{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Citations:\n",
        "# 1) MobileNet V2, https://huggingface.co/docs/transformers/model_doc/mobilenet_v2. "
      ],
      "metadata": {
        "id": "NIFOI2eUFis_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from natsort import natsorted\n",
        "import re\n",
        "import os\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "# from skimage import io\n",
        "import skimage\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import torch as t\n",
        "import torch.cuda as cuda\n",
        "import torch.optim as optimizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, transform\n",
        "from scipy import io\n",
        "from torchsummary import summary\n",
        "import csv\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Set the scenario number\n",
        "scenario_numx = 5\n",
        "scenario_num = scenario_numx\n",
        "\n",
        "all_cars = True\n",
        "\n",
        "# Set the working directory appropriately in this variable\n",
        "directory = '/content/gdrive/MyDrive/Dataset 2/Dataset/scenario' + str(scenario_numx) + '/resources'\n",
        "directory = directory + '/'\n",
        "\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Pdzd2SxY-r1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "path = directory + 'camera_data'\n",
        "filenames = os.listdir(path)\n",
        "original_list = [os.path.join(path, filename) for filename in filenames]\n",
        "full_path_filenames = natsorted(original_list)\n",
        "files = os.listdir(path)\n",
        "files = natsorted(files)\n"
      ],
      "metadata": {
        "id": "jk3wOGF4CSJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesserac"
      ],
      "metadata": {
        "id": "ElaEs6dHIt0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  MobileNet V2, https://huggingface.co/docs/transformers/model_doc/mobilenet_v2. \n",
        "from transformers import AutoImageProcessor, MobileNetV2ForSemanticSegmentation, PoolFormerConfig, PoolFormerModel, SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "checkpoint = \"nvidia/mit-b0\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)\n",
        "\n",
        "\n",
        "configuration = PoolFormerConfig(do_reize =False)\n",
        "model = PoolFormerModel(configuration)\n",
        "\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\")\n",
        "# image_processor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
        "model = MobileNetV2ForSemanticSegmentation.from_pretrained(\"google/deeplabv3_mobilenet_v2_1.0_513\")\n",
        "for _,i in enumerate(full_path_filenames):\n",
        "  if _%25==0:\n",
        "    print(_)\n",
        "  image = Image.open(i)\n",
        "\n",
        "  inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "  logits = outputs.logits\n",
        "  logits=logits[0]  \n",
        "  predicted_label = logits.argmax(-3)\n",
        "  unique, counts = torch.unique(predicted_label, return_counts=True)\n",
        "\n",
        "  predicted_label= np.where(predicted_label >0.05,1.0,0.0)\n",
        "  predicted_label = (predicted_label * 255).round().astype(np.uint8)\n",
        "  im = Image.fromarray(predicted_label)\n",
        "  im = im.convert(\"L\")\n",
        "  # print('files[_]',files[_])\n",
        "  im.save(directory + 'MobileNet masks new/' + files[_])\n",
        "\n"
      ],
      "metadata": {
        "id": "6LDofONaGpzF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}